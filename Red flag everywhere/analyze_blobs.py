#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
analyze_blobs.py

å‚è€ƒä½ ç»™çš„ç¤ºä¾‹ä»£ç ï¼Œé‡å†™ä¸ºæ›´ç¨³å¥çš„å®Œæ•´è„šæœ¬ï¼š
- å¯ä»ä¸¤ç§æ¥æºè¯»å–æ•°æ®ï¼š
  1) å•ä¸ª result.jsonï¼ˆå« {"blobs": [...]}ï¼‰
  2) data/ ç›®å½•ä¸‹çš„ blob_batch_*.jsonï¼ˆè‡ªåŠ¨éå†å¹¶è§£ææ··åˆ JSON/NDJSONï¼‰
- è®¡ç®—ç›¸é‚» blob å‘å¸ƒé—´éš”ï¼Œå¹¶è¾“å‡ºï¼š
  - æ—¶é—´çº¿å›¾ï¼ˆéšæ—¶é—´çš„é—´éš”ï¼Œæ ‡å‡ºå¼‚å¸¸ç‚¹ï¼Œå·¦ä¸Šè§’æ‘˜è¦æ¡†ï¼‰
  - ç›´æ–¹å›¾ï¼ˆé—´éš”åˆ†å¸ƒï¼‰
  - Markdown æŠ¥å‘Šï¼ˆå«ç»Ÿè®¡ä¸â€œæ˜¾è‘—é—´éš”â€è¡¨ï¼‰
  - è¯æ˜åˆ—è¡¨ CSVï¼ˆé€æ¡é—´éš”å¤æ ¸ï¼‰
- å¼‚å¸¸åˆ¤å®šé»˜è®¤ä¸º mean + 2*stdï¼ˆä¸ç¤ºä¾‹ä¸€è‡´ï¼‰ï¼Œå¯é€šè¿‡å‚æ•°è°ƒæ•´
- ä½¿ç”¨ Matplotlib éäº¤äº’åç«¯ Aggï¼Œé¿å… Windows/å‘½ä»¤è¡Œå¡ä½

ç”¨æ³•ï¼š
  python analyze_blobs.py                      # é»˜è®¤ä» ./result.json è¯»å–ï¼Œå¦åˆ™é€€å› ./data/*.json
  python analyze_blobs.py --result_json my.json
  python analyze_blobs.py --data_dir data --out_dir output --std_k 2.5

ä¾èµ–ï¼šnumpyã€matplotlib
"""

import argparse
import csv
import glob
import json
import math
import os
import sys
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Dict, List, Tuple

import numpy as np

# â€”â€” å…³é”®ï¼šéäº¤äº’å¼åç«¯ï¼Œé˜²æ­¢å‘½ä»¤è¡Œç¯å¢ƒå¡ä½ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ pyplot ä¹‹å‰è®¾ç½®ï¼‰
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
plt.ioff()
import matplotlib.dates as mdates


# ============================== æ•°æ®ç»“æ„ ==============================

@dataclass
class BlobRecord:
    id: Any
    time: datetime   # ç»Ÿä¸€ä¸ºâ€œUTC æ— æ—¶åŒºï¼ˆnaiveï¼‰æ—¶é—´â€ï¼Œä¾¿äºç”»å›¾
    height: int
    signer: str


# ============================== è§£æå·¥å…· ==============================

def parse_timestamp_to_utc_naive(s: str) -> datetime:
    """
    è§£æ ISO8601 æ—¶é—´ï¼ˆå…¼å®¹ä»¥ 'Z' ç»“å°¾æˆ–å¸¦åç§»é‡ï¼‰ï¼Œç»Ÿä¸€è½¬ä¸º UTC å¹¶å»æ‰ tzinfoï¼ˆnaiveï¼‰ã€‚
    """
    try:
        s = s.strip()
        if s.endswith("Z"):
            s = s[:-1] + "+00:00"
        dt = datetime.fromisoformat(s)
        if dt.tzinfo is None:
            # è§†ä¸º UTC
            return dt
        else:
            return dt.astimezone(timezone.utc).replace(tzinfo=None)
    except Exception:
        return None


def coalesce(d: Dict[str, Any], keys: List[str], default=None):
    for k in keys:
        cur = d
        ok = True
        for p in k.split("."):
            if isinstance(cur, dict) and p in cur:
                cur = cur[p]
            else:
                ok = False
                break
        if ok:
            return cur
    return default


def json_fragments_to_list(text: str) -> List[Dict[str, Any]]:
    """
    å°†æ–‡æœ¬å°½é‡è§£æä¸ºå¯¹è±¡åˆ—è¡¨ï¼šæ”¯æŒ JSON æ•°ç»„ã€å•å¯¹è±¡ã€æ‹¼æ¥å¯¹è±¡ã€NDJSONã€‚
    """
    text_stripped = text.strip()
    try:
        obj = json.loads(text_stripped)
        if isinstance(obj, list):
            return obj
        if isinstance(obj, dict):
            return [obj]
    except json.JSONDecodeError:
        pass

    decoder = json.JSONDecoder()
    idx, n = 0, len(text_stripped)
    out: List[Dict[str, Any]] = []
    while idx < n:
        while idx < n and text_stripped[idx] in [',', ' ', '\t', '\r', '\n']:
            idx += 1
        if idx >= n:
            break
        try:
            obj, off = decoder.raw_decode(text_stripped, idx)
            if isinstance(obj, dict):
                out.append(obj)
            elif isinstance(obj, list):
                out.extend([x for x in obj if isinstance(x, dict)])
            idx = off
        except json.JSONDecodeError:
            # å°è¯• NDJSON
            nd: List[Dict[str, Any]] = []
            for line in text.splitlines():
                s = line.strip().rstrip(',')
                if not s:
                    continue
                try:
                    obj = json.loads(s)
                    if isinstance(obj, dict):
                        nd.append(obj)
                    elif isinstance(obj, list):
                        nd.extend([x for x in obj if isinstance(x, dict)])
                except json.JSONDecodeError:
                    continue
            if nd:
                return nd
            break
    return out


def normalize_record(raw: Dict[str, Any]) -> BlobRecord:
    """
    ç»Ÿä¸€æå– idã€timeã€heightã€signerã€‚
    time ä¼˜å…ˆ 'time'ï¼Œå›é€€ 'tx.time'ã€‚è½¬æ¢ä¸º UTC-naiveã€‚
    """
    blob_id = coalesce(raw, ["id", "tx.id", "commitment", "tx.hash"])
    t = coalesce(raw, ["time", "tx.time"])
    if not t:
        raise ValueError("missing 'time'/'tx.time'")
    dt = parse_timestamp_to_utc_naive(str(t))
    if dt is None:
        raise ValueError(f"bad time format: {t}")

    height = coalesce(raw, ["height", "tx.height"])
    try:
        height = int(height) if height is not None else -1
    except Exception:
        height = -1

    signer = coalesce(raw, ["signer.hash", "signer.address", "signer"], "")
    return BlobRecord(id=blob_id, time=dt, height=height, signer=str(signer))


# ============================== æ•°æ®åŠ è½½ ==============================

def load_from_result_json(path: str) -> List[BlobRecord]:
    blobs_raw: List[Dict[str, Any]] = []
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict) and "blobs" in data and isinstance(data["blobs"], list):
            blobs_raw = data["blobs"]
        elif isinstance(data, list):
            blobs_raw = data
        else:
            print(f"[WARN] {path} æ ¼å¼ä¸å« 'blobs' åˆ—è¡¨ï¼Œå°è¯•ä½œä¸ºå¯¹è±¡/æ•°ç»„è¯»å–ã€‚", file=sys.stderr)
            blobs_raw = data.get("items", []) if isinstance(data, dict) else blobs_raw
    except Exception as e:
        print(f"[WARN] æ‰“å¼€ {path} å¤±è´¥ï¼š{e}", file=sys.stderr)
        return []

    out: List[BlobRecord] = []
    for o in blobs_raw:
        try:
            out.append(normalize_record(o))
        except Exception as e:
            print(f"[WARN] è·³è¿‡1æ¡è®°å½•ï¼ˆ{e}ï¼‰", file=sys.stderr)
    return out


def load_from_data_dir(data_dir: str) -> List[BlobRecord]:
    pattern = os.path.join(data_dir, "blob_batch_*.json")
    files = sorted(glob.glob(pattern))
    if not files:
        print(f"[WARN] æœªåœ¨ {data_dir} æ‰¾åˆ° blob_batch_*.json", file=sys.stderr)
    out: List[BlobRecord] = []
    for fp in files:
        try:
            with open(fp, "r", encoding="utf-8") as f:
                text = f.read()
            for obj in json_fragments_to_list(text):
                try:
                    out.append(normalize_record(obj))
                except Exception as e:
                    print(f"[WARN] è·³è¿‡æ— æ•ˆè®°å½•ï¼ˆ{fp}ï¼‰ï¼š{e}", file=sys.stderr)
        except Exception as e:
            print(f"[WARN] è¯»å–å¤±è´¥ {fp}ï¼š{e}", file=sys.stderr)
    return out


def load_blobs_auto(result_json: str, data_dir: str) -> List[BlobRecord]:
    if result_json and os.path.exists(result_json):
        recs = load_from_result_json(result_json)
        if recs:
            return recs
    # å›é€€ data_dir
    return load_from_data_dir(data_dir)


# ============================== ç»Ÿè®¡ä¸æ£€æµ‹ ==============================

def compute_gaps(records: List[BlobRecord]) -> Tuple[List[float], List[datetime]]:
    """
    è¿”å›ï¼šgaps_secondsï¼ˆlist[float]ï¼‰ï¼Œtimestamps_sortedï¼ˆlist[datetime]ï¼‰
    gaps[i] = timestamps[i] ä¸ timestamps[i-1] çš„å·®ï¼ˆç§’ï¼‰ï¼Œå¯¹åº”äº timestamps[i]
    """
    if not records:
        return [], []
    recs = sorted(records, key=lambda r: (r.time, r.height))
    ts = [r.time for r in recs]
    if len(ts) < 2:
        return [], ts

    gaps = []
    for i in range(1, len(ts)):
        gaps.append((ts[i] - ts[i-1]).total_seconds())
    return gaps, ts


def analyze_gaps(gaps: List[float], std_k: float = 2.0) -> Dict[str, Any]:
    """
    ç»Ÿè®¡ + å¼‚å¸¸ï¼ˆé˜ˆå€¼ = mean + std_k * stdï¼Œä¸ç¤ºä¾‹ä¿æŒä¸€è‡´ï¼‰
    """
    arr = np.asarray(gaps, dtype=float)
    mean_sec = float(np.mean(arr))
    median_sec = float(np.median(arr))
    std_sec = float(np.std(arr))
    thr = mean_sec + std_k * std_sec
    outliers = arr[arr > thr]
    return {
        "total_gaps": int(arr.size),
        "mean_gap_seconds": mean_sec,
        "median_gap_seconds": median_sec,
        "std_gap_seconds": std_sec,
        "outlier_threshold": float(thr),
        "outliers": outliers,
        "outlier_count": int(outliers.size),
        "max_gap_seconds": float(np.max(arr)),
        "min_gap_seconds": float(np.min(arr)),
    }


# ============================== è¾“å‡ºï¼šCSV / æŠ¥å‘Š ==============================

def save_proof_list_csv(path: str, records_sorted: List[BlobRecord], gaps: List[float]):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow([
            "index",
            "prev_time_utc", "next_time_utc",
            "gap_seconds", "gap_minutes", "gap_hours",
            "prev_id", "next_id", "prev_height", "next_height",
            "prev_signer", "next_signer"
        ])
        for i in range(1, len(records_sorted)):
            a, b = records_sorted[i-1], records_sorted[i]
            g = gaps[i-1]
            w.writerow([
                i-1,
                a.time.isoformat(sep=' '), b.time.isoformat(sep=' '),
                f"{g:.6f}", f"{g/60.0:.6f}", f"{g/3600.0:.6f}",
                a.id, b.id, a.height, b.height, a.signer, b.signer
            ])


def generate_report_md(path: str,
                       records: List[BlobRecord],
                       timestamps: List[datetime],
                       gaps: List[float],
                       analysis: Dict[str, Any],
                       namespace_hint: str = "N/A"):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    now = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
    if timestamps:
        first_ts, last_ts = min(timestamps), max(timestamps)
        period_days = (last_ts - first_ts).total_seconds() / 86400.0
    else:
        first_ts = last_ts = None
        period_days = 0.0

    lines = []
    lines.append("# Celestia Blob Posting Consistency Analysis\n")
    lines.append(f"**Analysis Date:** {now} UTC  \n**Namespace:** `{namespace_hint}`\n")
    lines.append("## Executive Summary\n")

    if not gaps:
        lines.append("âŒ **Insufficient data** - Need at least 2 blobs to analyze consistency.\n")
    else:
        cv = analysis["std_gap_seconds"] / analysis["mean_gap_seconds"] if analysis["mean_gap_seconds"] > 0 else float("inf")
        if cv < 0.5:
            consistency = "âœ… **CONSISTENT** - Low variability in posting intervals"
        elif cv < 1.0:
            consistency = "âš ï¸ **MODERATELY CONSISTENT** - Some variability in posting intervals"
        else:
            consistency = "âŒ **INCONSISTENT** - High variability in posting intervals"
        lines.append(f"{consistency}\n")
        if analysis["outlier_count"] > 0:
            lines.append(f"**{analysis['outlier_count']} significant gaps** detected that are much longer than usual.\n")
        else:
            lines.append("**No significant gaps** detected - posting appears regular.\n")

    lines.append("## Data Overview\n")
    lines.append(f"- **Total Blobs:** {len(records)}")
    lines.append(f"- **Time Gaps Analyzed:** {len(gaps)}")
    lines.append(f"- **Analysis Period:** {period_days:.1f} days\n")
    if timestamps:
        lines.append(f"- **First Blob:** {first_ts.strftime('%Y-%m-%d %H:%M:%S')} UTC")
        lines.append(f"- **Last Blob:** {last_ts.strftime('%Y-%m-%d %H:%M:%S')} UTC\n")

    if gaps:
        lines.append("## Gap Statistics\n")
        lines.append(f"- **Average Gap:** {analysis['mean_gap_seconds']:.0f} s ({analysis['mean_gap_seconds']/3600:.2f} h)")
        lines.append(f"- **Median Gap:** {analysis['median_gap_seconds']:.0f} s ({analysis['median_gap_seconds']/3600:.2f} h)")
        lines.append(f"- **Std Dev:** {analysis['std_gap_seconds']:.0f} s ({analysis['std_gap_seconds']/3600:.2f} h)")
        lines.append(f"- **Shortest Gap:** {analysis['min_gap_seconds']:.0f} s")
        lines.append(f"- **Longest Gap:** {analysis['max_gap_seconds']:.0f} s ({analysis['max_gap_seconds']/3600:.2f} h)\n")

        # æ˜¾è‘—é—´éš”è¡¨ï¼ˆè¶…è¿‡é˜ˆå€¼ï¼‰
        thr = analysis["outlier_threshold"]
        lines.append("## Significant Gaps Identified\n")
        lines.append(f"**Outlier Threshold:** {thr:.0f} s ({thr/3600:.2f} h)\n")
        lines.append("| Gap # | Duration (s) | Hours | Days | Before Time (UTC) | After Time (UTC) |")
        lines.append("|------:|-------------:|------:|-----:|-------------------|------------------|")
        shown = 0
        for i, g in enumerate(gaps, 1):
            if g > thr:
                shown += 1
                bt = timestamps[i-1].strftime("%Y-%m-%d %H:%M:%S")
                at = timestamps[i].strftime("%Y-%m-%d %H:%M:%S")
                lines.append(f"| {i} | {g:.0f} | {g/3600:.1f} | {g/86400:.1f} | {bt} | {at} |")
                if shown >= 15:
                    lines.append(f"\n*â€¦ and more ({analysis['outlier_count'] - shown} hidden)*")
                    break

    lines.append("\n## Visual Analysis\n")
    lines.append("![Gaps Over Time](gaps_over_time.png)")
    lines.append("![Gap Distribution](blob_gap_histogram.png)\n")
    lines.append("---\n*Report generated automatically.*\n")

    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))


# ============================== ç»˜å›¾ ==============================

def create_time_plot(timestamps: List[datetime],
                     gaps: List[float],
                     analysis: Dict[str, Any],
                     save_path: str):
    """
    ç”Ÿæˆâ€œéšæ—¶é—´çš„é—´éš”â€å›¾ï¼ˆå‚è€ƒç¤ºä¾‹ï¼‰ï¼šæµ…è“æŠ˜çº¿ + ç»¿è‰²ä¸­ä½çº¿ + çº¢è‰²å¼‚å¸¸ç‚¹ï¼ˆå¸¦æ ‡æ³¨ï¼‰+ å·¦ä¸Šè§’æ‘˜è¦æ¡†ã€‚
    """
    if len(gaps) == 0:
        return

    gaps_h = [g/3600.0 for g in gaps]
    gap_ts = timestamps[1:]  # é—´éš”å¯¹åº”â€œåä¸€æ¡â€çš„æ—¶é—´

    plt.figure(figsize=(16, 9))

    # æ­£å¸¸æŠ˜çº¿
    plt.plot(gap_ts, gaps_h, 'o-', color='lightsteelblue',
             markersize=3, linewidth=1, alpha=0.6, label='Normal gaps')

    # ä¸­ä½æ•°çº¿
    median_h = analysis["median_gap_seconds"] / 3600.0
    plt.axhline(median_h, color='green', linestyle='-', linewidth=2,
                alpha=0.9, label=f"Median: {median_h:.2f}h")

    # å¼‚å¸¸ç‚¹ï¼ˆ> mean + k*stdï¼‰
    thr = analysis["outlier_threshold"]
    outliers = []
    for i, g in enumerate(gaps):
        if g > thr:
            gh = g / 3600.0
            plt.scatter(gap_ts[i], gh, color='red', s=80, zorder=5,
                        alpha=0.9, edgecolors='darkred', linewidth=1)
            # æ³¨é‡Š
            plt.annotate(f"{gh:.1f}h",
                         (gap_ts[i], gh),
                         xytext=(10, 10), textcoords='offset points',
                         bbox=dict(boxstyle='round,pad=0.3', facecolor='red', alpha=0.75),
                         fontsize=9, color='white', weight='bold',
                         arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))
            outliers.append((gap_ts[i], gh))

    # å›¾ä¾‹ä¸­çš„â€œOutliersâ€æ ·ä¾‹
    if outliers:
        plt.scatter([], [], color='red', s=80, alpha=0.9, edgecolors='darkred',
                    linewidth=1, label=f'Outliers ({len(outliers)} found)')

    # y è½´ä» 0 å¼€å§‹ï¼Œé¡¶éƒ¨ç•™ç™½
    ymax = max(gaps_h) if gaps_h else 1.0
    plt.ylim(bottom=0, top=ymax * 1.1)

    # æ‘˜è¦æ¡†
    summary_text = (
        f"Summary:\n"
        f"â€¢ Total gaps analyzed: {len(gaps)}\n"
        f"â€¢ Median gap: {median_h:.2f} hours\n"
        f"â€¢ Outliers detected: {len(outliers)}\n"
        f"â€¢ Largest gap: {ymax:.1f}h"
    )
    plt.text(0.02, 0.98, summary_text, transform=plt.gca().transAxes,
             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8),
             va='top', fontsize=10, family='monospace')

    # è½´ä¸æ ‡é¢˜
    ax = plt.gca()
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))
    plt.xticks(rotation=30)
    plt.xlabel('Date (UTC)', fontsize=12)
    plt.ylabel('Gap Between Blobs (hours)', fontsize=12)
    plt.title('Celestia Blob Posting Gaps Over Time\n(Red dots show abnormally long gaps)', fontsize=14, pad=18)
    plt.grid(True, alpha=0.3, linestyle='--')
    plt.legend(loc='lower right', fontsize=11)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()


def create_histogram(gaps: List[float], save_path: str):
    """
    ç›´æ–¹å›¾ï¼ˆå•ä½ï¼šå°æ—¶ï¼‰ï¼Œå¸¦å‡å€¼/ä¸­ä½æ•°/95åˆ†ä½å‚è€ƒçº¿ï¼›å¯¹é‡å°¾åˆ†å¸ƒè‡ªåŠ¨åˆ‡æ¢å¯¹æ•° y è½´ã€‚
    """
    if len(gaps) == 0:
        return
    vals = np.array(gaps, dtype=float) / 3600.0

    # Freedmanâ€“Diaconis ä¼°è®¡ç®±æ•°å¹¶é™åˆ¶ä¸Šé™
    def fd_bins(x: np.ndarray) -> int:
        if x.size < 2:
            return 10
        q1, q3 = np.percentile(x, [25, 75])
        iqr = q3 - q1
        if iqr <= 0:
            return 50
        h = 2 * iqr * (x.size ** (-1/3))
        if h <= 0:
            return 50
        return max(10, min(200, int(math.ceil((x.max() - x.min()) / h))))

    bins = fd_bins(vals)

    plt.figure(figsize=(12, 7))
    n, b, _ = plt.hist(vals, bins=bins, alpha=0.9)

    # åæ€è‡ªåŠ¨ä½¿ç”¨å¯¹æ•° y
    p50, p99 = np.percentile(vals, [50, 99])
    if p50 > 0 and (p99 / p50) > 50:
        plt.yscale('log')

    mean = float(np.mean(vals))
    median = float(np.median(vals))
    p95 = float(np.percentile(vals, 95))
    plt.axvline(mean, linestyle='-', linewidth=1.8, label=f"Mean â‰ˆ {mean:.2f}")
    plt.axvline(median, linestyle='--', linewidth=1.8, label=f"Median â‰ˆ {median:.2f}")
    plt.axvline(p95, linestyle=':', linewidth=1.8, label=f"95th pct â‰ˆ {p95:.2f}")

    plt.xlabel("Gap between consecutive blobs (hours)")
    plt.ylabel("Frequency" + (" (log scale)" if plt.gca().get_yscale() == 'log' else ""))
    plt.title("Histogram of Blob Publish Gaps", pad=12)
    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    plt.legend()
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()


# ============================== ä¸»æµç¨‹ ==============================

def main():
    ap = argparse.ArgumentParser(description="Analyze Celestia blob publish gaps and visualize (timeline + histogram).")
    ap.add_argument("--result_json", type=str, default="result.json", help="åŒ…å« {'blobs': [...]} çš„ JSON è·¯å¾„")
    ap.add_argument("--data_dir", type=str, default="data", help="å¤‡ç”¨æ•°æ®ç›®å½•ï¼ˆblob_batch_*.jsonï¼‰")
    ap.add_argument("--out_dir", type=str, default="output", help="è¾“å‡ºç›®å½•")
    ap.add_argument("--std_k", type=float, default=2.0, help="å¼‚å¸¸é˜ˆå€¼ = mean + std_k * stdï¼ˆé»˜è®¤ 2.0ï¼‰")
    ap.add_argument("--namespace", type=str, default="N/A", help="æŠ¥å‘Šä¸­æ˜¾ç¤ºçš„ namespaceï¼ˆå¯é€‰ï¼‰")
    args = ap.parse_args()

    # åŠ è½½æ•°æ®ï¼ˆä¼˜å…ˆ result.jsonï¼‰
    records = load_blobs_auto(args.result_json, args.data_dir)
    if not records:
        print("âŒ æœªåŠ è½½åˆ°ä»»ä½• blob è®°å½•ã€‚è¯·æ£€æŸ¥ result.json æˆ– data ç›®å½•ã€‚")
        return

    # è®¡ç®—é—´éš”
    gaps, timestamps = compute_gaps(records)
    if not gaps:
        print("âŒ è®°å½•ä¸è¶³ï¼ˆ<2ï¼‰æˆ–ç¼ºå°‘å¯è§£æçš„æ—¶é—´å­—æ®µã€‚")
        return

    print(f"Loaded {len(records)} blobs  |  Computed {len(gaps)} gaps")

    # ç»Ÿè®¡ + å¼‚å¸¸
    analysis = analyze_gaps(gaps, std_k=args.std_k)

    # è¾“å‡ºè·¯å¾„
    os.makedirs(args.out_dir, exist_ok=True)
    out_time = os.path.join(args.out_dir, "gaps_over_time.png")
    out_hist = os.path.join(args.out_dir, "blob_gap_histogram.png")
    out_md = os.path.join(args.out_dir, "blob_consistency_report.md")
    out_csv = os.path.join(args.out_dir, "proof_list.csv")

    # å¯è§†åŒ–
    print("Drawing timeline plot...")
    create_time_plot(timestamps, gaps, analysis, save_path=out_time)
    print("Drawing histogram...")
    create_histogram(gaps, save_path=out_hist)

    # è¯æ˜åˆ—è¡¨ + æŠ¥å‘Š
    print("Saving proof list & report...")
    records_sorted = sorted(records, key=lambda r: (r.time, r.height))
    save_proof_list_csv(out_csv, records_sorted, gaps)
    generate_report_md(out_md, records_sorted, timestamps, gaps, analysis, namespace_hint=args.namespace)

    print("âœ… Done.")
    print(f"ğŸ“ˆ Timeline: {out_time}")
    print(f"ğŸ“Š Histogram: {out_hist}")
    print(f"ğŸ“„ Report: {out_md}")
    print(f"ğŸ§¾ Proof CSV: {out_csv}")
    print(f"Outliers (> mean + {args.std_k}*std): {analysis['outlier_count']}  |  Largest gap: {analysis['max_gap_seconds']/3600:.2f} h")


if __name__ == "__main__":
    main()
